Initializing AMReX (25.01-9-ge761abff95af)...
MPI initialized with 4 MPI processes
MPI initialized with thread support level 0
Initializing CUDA...
Multiple GPUs are visible to each MPI rank. This is usually not an issue. But this may lead to incorrect or suboptimal rank-to-GPU mapping.!
CUDA initialized with 4 devices.
AMReX (25.01-9-ge761abff95af) initialized

 FFT size: 512 512 256   # of proc. 4

    Warm-up: 0.031158225
    Test # 0: 0.011992383
    Test # 1: 0.01198581
    Test # 2: 0.012425421
    Test # 3: 0.011179521
    Test # 4: 0.011967234
    Test # 5: 0.012034034
    Test # 6: 0.012027622
    Test # 7: 0.012024625
    Test # 8: 0.012035016
    Test # 9: 0.011976903
    Warm-up: 0.025734752
    Test # 0: 0.011649339
    Test # 1: 0.009719366
    Test # 2: 0.009861339
    Test # 3: 0.009518206
    Test # 4: 0.009913821
    Test # 5: 0.01049726
    Test # 6: 0.00999069
    Test # 7: 0.009894874
    Test # 8: 0.01030613
    Test # 9: 0.010131032
  armex pencil time: 0.0119648569
  amrex slab   time: 0.0101482057

Total GPU global memory (MB) spread across MPI: [40326 ... 40326]
Free  GPU global memory (MB) spread across MPI: [8994 ... 9386]
[The         Arena] space (MB) allocated spread across MPI: [30244 ... 30244]
[The         Arena] space (MB) used      spread across MPI: [0 ... 0]
[The Managed Arena] space (MB) allocated spread across MPI: [8 ... 8]
[The Managed Arena] space (MB) used      spread across MPI: [0 ... 0]
[The  Pinned Arena] space (MB) allocated spread across MPI: [8 ... 8]
[The  Pinned Arena] space (MB) used      spread across MPI: [0 ... 0]
[The   Comms Arena] space (MB) allocated spread across MPI: [264 ... 650]
[The   Comms Arena] space (MB) used      spread across MPI: [0 ... 0]
AMReX (25.01-9-ge761abff95af) finalized
